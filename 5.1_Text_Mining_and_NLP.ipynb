{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\"> Text Mining </h2>\n",
    "\n",
    "<br>\n",
    "Text Mining is the process of deriving meaningful information <br> from natural language text.\n",
    "\n",
    "The overall goal is, essentially to turn text into data for <br>\n",
    "analysis, via application of Natural Language(NLP).\n",
    "\n",
    "NLP is a component of text mining that performs a special kind of <br> linguistic analysis that essentially helps a machine \"read\" text.\n",
    "\n",
    "\n",
    "What is NLTK?\n",
    "NLTK stands for Natural Language Toolkit. This toolkit is one of <br> the most powerful NLP libraries which contains packages <br>\n",
    "to make machines understand human language \n",
    "\n",
    "pip install nltk\n",
    "\n",
    "<h4> Applications of NLP </h4>\n",
    "\n",
    "- Sentimenal Analysis\n",
    "- Speech Recognition\n",
    "- Chatbot\n",
    "- Machine Translation\n",
    "- Spell Checking\n",
    "- Keyword Search\n",
    "- Advertising matching\n",
    "\n",
    "<h4> Tutorials </h4>\n",
    "\n",
    "- Tokenization\n",
    "- Stop Words\n",
    "- Stemming\n",
    "- POS - Part of speech\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'movie_reviews', 'movie_reviews.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus** -> A collection of written texts, ex- medical journals, parliament debates\n",
    "\n",
    "**downloading any corpus**\n",
    "\n",
    "nltk.download(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt') // will download tokenization\n",
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg.fileids() # all the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "# emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in emma[:500]:\n",
    "#     print(word, sep=\" \", end= \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\"> tokenization </h3>\n",
    "\n",
    "- Break a complex sentence into words\n",
    "- Understand the importance of each of the words with respect to the sentence.\n",
    "- produce a structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In Brazil they drive on the right-hand side of the road.', 'Brazil has a large coastline on the eastern side of South America']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'in': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in token:\n",
    "    fdist[word.lower()] += 1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('brazil', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('of', 2),\n",
       " ('in', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of paragraph\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "b_token = blankline_tokenize(text)\n",
    "len(b_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Bigrams** - token of two consecutive written words known as Bigram\n",
    "- **Trigrams** - Tokens of three consecutive written words known as Tigram\n",
    "- **Ngrams** - Tokens of any number of consecutive written words known as Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'Brazil'),\n",
       " ('Brazil', 'they'),\n",
       " ('they', 'drive'),\n",
       " ('drive', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'right-hand'),\n",
       " ('right-hand', 'side'),\n",
       " ('side', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'road'),\n",
       " ('road', '.'),\n",
       " ('.', 'Brazil'),\n",
       " ('Brazil', 'has'),\n",
       " ('has', 'a'),\n",
       " ('a', 'large'),\n",
       " ('large', 'coastline'),\n",
       " ('coastline', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'eastern'),\n",
       " ('eastern', 'side'),\n",
       " ('side', 'of'),\n",
       " ('of', 'South'),\n",
       " ('South', 'America')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_bigrams = list(bigrams(token))\n",
    "token_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\"> Stop Words </h3> <br>\n",
    "Stopwords are the most common words in any natural language. For the purpose <br>\n",
    "of analyzing text data and building NLP models, these stopwords might not add <br>\n",
    "much value to the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = [l for l in token if l not in stop_words]\n",
    "# print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\"> Stemming </h3> <br>\n",
    "Normalize words into its base form or root form \n",
    "\n",
    "example - Affect(root word)- affection, affects, affectation, affected, affecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plural words -\n",
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization','sensational', 'traditional', 'reference', 'colonizer','plotted']\n",
    "\n",
    "# for word in words:\n",
    "#     print(word, \":\", pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\"> POS tagging - Part of Speech </h3>\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling <br> them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging.\n",
    "\n",
    "**list of pos tag** - > https://www.sketchengine.eu/penn-treebank-tagset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
